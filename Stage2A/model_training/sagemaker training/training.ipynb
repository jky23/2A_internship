{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to train a Segmentation model on AWS Sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Create a AWS Sagemaker Notebook](#1)\n",
    "2. [Create an AWS S3 Bucket](#2)\n",
    "3. [Setup](#3)\n",
    "4. [Data](#4)\n",
    "5. [Train the model](#5)\n",
    "6. [Host](#6)\n",
    "7. [Clean up](#7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create a AWS Sagemaker Notebook Instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You first need to have an AWS account. You can refer to this page for further explanations [Create a Sagemaker Notebook](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html). \n",
    "\n",
    "During the creation of the notebook, you will need to:\n",
    " - Have an IAM Role to allow interactions between the notebook and the S3 Bucket\n",
    " - Choose an instance type for your Notebook. Since you will need to train a model that requires GPU instances, you need to choose a GPU instance. The cheapest instance for training is *ml.p3.2xlarge*. Refer to [this](https://aws.amazon.com/fr/sagemaker/pricing/instance-types) to see the instances available in your region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create an AWS S3 Bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train your model, you need to store your datasets and your training process will produces a model (*model.pth*) or some outputs that you need to store. For that purpose, you can create a S3 Bucket by following the steps on this page. [Create a S3 Bucket](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/create-bucket.html).\n",
    "\n",
    "For example, assume that you create a S3 Bucket name *sagemaker-inspi-data*, and create a folder *data/* where you store your training data in *train/* folder and your validation data in *val* folder. Both folders have 02 subfolders *image* and *mask* to store the images and their labels.\n",
    "We can also create a folder in the S3 Bucket to store results of the training process. Let's call it *output/*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Setup\n",
    "\n",
    "\n",
    "We first need to create a Sagemaker Session and specify the IAM role that will be use. It is the role that allow the notebook to access your data stored in the S3 and your code If the role use to create the Sagemaker notebook is different from the one use for the S3, replace the ```sagemaker.get_execution_role()``` by the appropriate role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Data\n",
    "\n",
    "Now you need to take the path of the inputs data (training and validation) in order to pass it to training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dir = 's3://sagemaker-inspi-data/data/train'\n",
    "testing_dir = 's3://sagemaker-inspi-data/data/test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you skip **Step 2** and prefer use the default bucket and upload your data in this bucket and uncomment the following cell.\n",
    "It will use the default bucket and upload your data in a folder *data/*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bucket = sagemaker_session.default_bucket()\n",
    "# prefix = 'sagemaker-inspi-data/pytorch'\n",
    "\n",
    "## Download the data and save it the folder 'data'\n",
    "\n",
    "# Upload the data in the S3 Bucket\n",
    "# training_dir = sagemaker_session.upload_data(path='data/train', bucket=bucket, key_prefix=prefix+'data/train')\n",
    "# testing_dir = sagemaker_session.upload_data(path='data/val', bucket=bucket, key_prefix=prefix+'/data/val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```input_data``` variable will take the path of your training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = {'training':training_dir, 'testing':testing_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Train the model\n",
    "\n",
    "### 5-1. Training file\n",
    "\n",
    "To train the model, you will need to run the training file ```train.py```. This script provides all the code we need for training and hosting a SageMaker model (model_fn function to load a model). The training script is very similar to a training script you might run outside of SageMaker, but you can access useful properties about the training environment through various environment variables, such as:\n",
    "\n",
    "   - SM_MODEL_DIR: A string representing the path to the directory to write model artifacts to. These artifacts are uploaded to S3 for model hosting.\n",
    "   - SM_NUM_GPUS: The number of gpus available in the current container.\n",
    "   - SM_CURRENT_HOST: The name of the current container on the container network.\n",
    "   - SM_HOSTS: JSON encoded list containing all the hosts .\n",
    "   - SM_CHANNEL_TRAINING: A string representing the path to the directory that contains training datasets\n",
    "   - SM_CHANNEL_TESTING: A string representing the path to the directory that contains testing datasets.\n",
    "   \n",
    "For more information about Sagemaker environment variables, refer to [Sagemaker Containers](https://github.com/aws/sagemaker-containers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Run the training job\n",
    "\n",
    "The PyTorch class allows us to run our training function as a training job on SageMaker infrastructure. We need to configure it with our training script, an IAM role, the number of training instances, the training instance type, and hyperparameters. \n",
    "\n",
    "This code was written with **PyTorch 1.6.0** but the latest version of PyTorch in Sagemaker is **1.5.0**, but this is not a problem. You need to specify it otherwise, the default version of PyTorch which is **0.4.1** will be used and this can cause many errors.\n",
    "\n",
    "If you want to make a distributed training you will need to specify more than 1 ML instances. As mentionned before, our code required a GPU instance, so the instances used will be *ml.p3.2xlarge*\n",
    "If you choose a CPU instance or a GPU instance that does not fit your data, you will be out of memory. You will need to reduce your batch size.\n",
    "\n",
    "If your training script is in a folder, you will also need to specify it with ```source_dir```.\n",
    "If you use other libraries or packages that are not installed by default to the PyTorch containers, you must include a *requirements.txt* file that list all the dependencies libraries. They will be installed automatically by running the training job.\n",
    "\n",
    "The hyperparameters parameter is a dict of values that will be passed to your training script. Our training script take a lot of hyperparameters -- you can see how to access these values by running ``` python3 train.py --help``` or opening the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_version = '1.5.0'  # Pytorch version for training\n",
    "trainfile = 'train.py'   # file where is the training script\n",
    "nb_ml_instances = 1   # the number of instances for training\n",
    "type_ml_instance = 'ml.p3.2xlarge'  # the type of instances for training\n",
    "#type_ml_instance = 'local' \n",
    "hyper_param = {'epochs': 100, \n",
    "               'backend': 'nccl',\n",
    "               'batch-size' : 10,\n",
    "               'workers':2,\n",
    "               'train-size': 225\n",
    "              }\n",
    "output_path = 's3://sagemaker-inspi-data/output'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use **TensorBoard** to visualize some information about your training. It will produce an event file that you can visualize with TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.debugger import TensorBoardOutputConfig\n",
    "\n",
    "tensorboard_output_config = TensorBoardOutputConfig(\n",
    "    s3_output_path='s3://sagemaker-inspi-data/output/data/emission'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the training job, now you need to create a PyTorch object and pass all the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(entry_point=trainfile,\n",
    "                    role=role,\n",
    "                    source_dir='code',\n",
    "                    output_path = output_path,\n",
    "                    framework_version=pytorch_version,\n",
    "                    train_instance_count=nb_ml_instances,\n",
    "                    train_instance_type=type_ml_instance,\n",
    "                    tensorboard_output_config=tensorboard_output_config,\n",
    "                    hyperparameters=hyper_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating your PyTorch Object, you can now fit your model to your input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the training, you will get a model in your S3 Bucket. You can download it in order to use it on other devices or services, or you can deploy it with Sagemaker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Host\n",
    "\n",
    "### 6-1. Create endpoint\n",
    "\n",
    "After training, we use the PyTorch estimator object to build and deploy a PyTorchPredictor. This creates a Sagemaker Endpoint -- a hosted prediction service that we can use to perform inference.\n",
    "\n",
    "We have implemented a ```model_fn``` function in the train.py script that is required. We are going to use default implementations of *input_fn, predict_fn, output_fn and transform_fn* defined in sagemaker-pytorch-containers.\n",
    "\n",
    "The arguments to the deploy function allow us to set the number and type of instances that will be used for the Endpoint. These do not need to be the same as the values we used for the training job. For example, you can train a model on a set of GPU-based instances, and then deploy the Endpoint to a fleet of CPU-based instances, but you need to make sure that you return or save your model as a cpu model similar to what we did in train.py. Here we will deploy the model to a single *ml.m5.xlarge* instance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = estimator.deploy(initial_instance_count=1, instance_type='ml.m5.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-2. Evaluate\n",
    "\n",
    "You can know use it to make some predictions with images. \n",
    "You can refer to ```test_1.py``` file to see how to use the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Clean up\n",
    "\n",
    "After you have finished with this example, remember to delete the prediction endpoint to release the instance(s) associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to avoid unnecessary costs, you will need to delete the ressources that you don't need anymore.\n",
    "Open the Amazon SageMaker console at https://console.aws.amazon.com/sagemaker/ and delete the following resources:\n",
    "\n",
    "1. **The endpoint**. Deleting the endpoint also deletes the ML compute instance or instances that support it.\n",
    "\n",
    "2. **The endpoint configuration.**\n",
    "\n",
    "3. **The model.**\n",
    "\n",
    "4. **The notebook instance. Before deleting the notebook instance, stop it.**\n",
    "\n",
    "5. **The S3 Bucket**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Additional ressources\n",
    "\n",
    "You can find additional ressources for further information:\n",
    "1. [Get Started with AWS Sagemaker](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-console.html)\n",
    "2. [Examples of notebooks with AWS Sagemaker](https://github.com/awslabs/amazon-sagemaker-examples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
